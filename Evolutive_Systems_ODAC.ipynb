{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evolutive Systems - ODAC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gillesvtsilvano/odac/blob/master/Evolutive_Systems_ODAC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVRJPpHB4mqY",
        "colab_type": "text"
      },
      "source": [
        "# Introdução\n",
        "\n",
        "<em>Online Divisive-Agglomerative Clustering - ODAC.</em>\n",
        "\n",
        "Baseado em \n",
        "RodriguesGamaPedroso.2008.HierarchicalClusteringofTime-SeriesDataStreams.pdf\n",
        "\n",
        "\n",
        "## Series temporais em agrupamentos de fluxos\n",
        "\n",
        "Agrupar variáveis que se comportem similarmente através do tempo.\n",
        "\n",
        "Objetivo é fornecer uma matriz booleanda $ M_{i, j, k}$, que representa se uma variável $M_i$ pertence ao cluster $M_j$ no instante $M_k$.\n",
        "\n",
        "## Medição incremental de dissimilaridade\n",
        "\n",
        "Distâncias são incrementalmente computadas utilizando o limite de Hoeffding e a correlação de Pearson.\n",
        "\n",
        "Tome a correlação entre as séries $a$ e $b$:\n",
        "\n",
        "$corr(a, b)=\\frac{P-\\frac{AB}{n}}{\\sqrt{A_2- \\frac{A^2}{n}} \\sqrt{B_2-\\frac{B^2}{n} }}$\n",
        "\n",
        "Onde,\n",
        "\n",
        "$A=\\sum{a_i}, B=\\sum{b_i}, C=\\sum{c_i}$\n",
        "\n",
        "No ODAC, é utilizado o _Rooted Normalized One-Minus-Correlation_ dado por:\n",
        "\n",
        "$rnomc(a, b)=\\sqrt{\\frac{1-corr(a,b)}{2}}$\n",
        "\n",
        "Considera-se o <em>diâmetro do cluster</em> a maior dissimilaridade entre duas series temporais pertencentes ao mesmo cluster.\n",
        "\n",
        "## Aumentando a Hierarquia\n",
        "\n",
        "O sistema incrementa as estatísticas suficientes para calcular a matriz de dissimilaridade. Quando processando um novo exemplo, somente as folhas são atualizadas.\n",
        "\n",
        "### Critério de divisão\n",
        "\n",
        "O critério é o limite de Hoeffding  que é independente de distribuiçõa de probabilidade. Depois de $n$ observações independentes de uma variável aleatória real $r$ de tamanho $R$, com confiança $1-\\delta$, a média de $r$ é pelo menos $\\bar{r}-\\epsilon$, onde $\\bar{r}$ é a média observada e $\\epsilon$\n",
        "\n",
        "\n",
        "$\\epsilon=\\sqrt{\\frac{R^2 ln(1/\\delta)}{2n}}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KMNW7pc7lVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rodrigoejcm/odac.git\n",
        "import os\n",
        "\n",
        "#os.chdir('odac')\n",
        "!cp odac/requirements.txt .\n",
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi-QfmBKUt7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "#!wget http://cs.joensuu.fi/sipu/datasets/s1.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjbq6Nbkk61K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('http://cs.joensuu.fi/sipu/datasets/s1.txt', sep='    ', names=['x', 'y'])\n",
        "df2 = pd.read_csv('http://cs.joensuu.fi/sipu/datasets/s2.txt', sep='    ', names=['x', 'y'])\n",
        "df3 = pd.read_csv('http://cs.joensuu.fi/sipu/datasets/s3.txt', sep='    ', names=['x', 'y'])\n",
        "df4 = pd.read_csv('http://cs.joensuu.fi/sipu/datasets/s4.txt', sep='    ', names=['x', 'y'])\n",
        "\n",
        "df = pd.concat([df1, df2, df3, df4])\n",
        "del df1, df2, df3, df4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdpAjZaJxMej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.iloc[2:5,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x13QHufR6Rmk",
        "colab_type": "text"
      },
      "source": [
        "#timeseries.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDCC2kWm6NEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import floor\n",
        "\n",
        "class Timeseries:\n",
        "\n",
        "  current_value = None   \n",
        "\n",
        "  def __init__(self, name, df):\n",
        "    self.name = name\n",
        "    self.idx = 0\n",
        "    self.df = df\n",
        "    self.next_val()\n",
        "\n",
        "    \n",
        "    #print(\"TS(\", self.idx, \")\", self.name , \" inicializado com valor: \", self.current_value )\n",
        "\n",
        "  def next_val(self):\n",
        "    #print(\">>> \", self.name, \": accessing idx \", self.idx)\n",
        "    #self.current_value = float('%.2f'%(self.strem_data_generator.next_sample()[0][0][0]))\n",
        "    self.current_value = self.df.values[self.idx]\n",
        "    self.idx = self.idx + 1\n",
        "    #print('>>> ', Timeseries.idx)\n",
        "    return self.current_value\n",
        "\n",
        "def generate_multi_timeseries(n):\n",
        "  dic_of_timeseries = {}\n",
        "  #df_size = floor(int(len(df.values) / n))\n",
        "  for i in range(n):\n",
        "      #dic_of_timeseries[\"S\"+str(i)] = Timeseries(name=\"S\"+str(i), df = df.iloc[(df_size*i):df_size*(i+1)-1])\n",
        "      dic_of_timeseries[\"S\"+str(i)] = Timeseries(name=\"S\"+str(i), df = df.sample(n))\n",
        "  return dic_of_timeseries\n",
        "\n",
        "\n",
        "def get_timeseries_next_value(dic_of_timeseries):\n",
        "  next_values = []\n",
        "  for key, value in dic_of_timeseries.items():\n",
        "      next_values.append((key, value.next_val()))\n",
        "  return next_values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbBMhQD1x-6g",
        "colab_type": "text"
      },
      "source": [
        "#tree.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v7svfeg6Ma1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from anytree import NodeMixin, RenderTree\n",
        "from itertools import combinations_with_replacement\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "from math import sqrt, log\n",
        "\n",
        "class Statistics:\n",
        "\n",
        "    def __init__(self, ts_quantity):\n",
        "\n",
        "        self.reset_sufficient_statistics(ts_quantity)\n",
        "        self.cluster_diameter = None\n",
        "        self.dist_dict_coef = {}\n",
        "        self.hoeffding_bound = None\n",
        "\n",
        "\n",
        "    def reset_sufficient_statistics(self, ts_quantity):\n",
        "        self.sum_dict = {}       # key: cluster number\n",
        "        self.prd_dict = {}       # key: tuple of two cluster numbers\n",
        "        self.corr_dict = {}      # key: tuple of two cluster numbers\n",
        "        self.rnomc_dict = {}     # key: tuple of two cluster numbers\n",
        "        self.n_of_instances = 0\n",
        "        for i in range(ts_quantity):\n",
        "            self.sum_dict[i] = 0.\n",
        "            for j in range(ts_quantity):\n",
        "                if j >= i:\n",
        "                    self.prd_dict[(i,j)] = 0.\n",
        "                    if j > i:\n",
        "                        self.corr_dict[(i,j)] = 0.\n",
        "                        self.rnomc_dict[(i,j)] = 0.\n",
        "\n",
        "    def print(self):\n",
        "        print(\"# n_of_instances = {}\".format(self.n_of_instances))\n",
        "        #print(\"# cluster_diameter = {}\".format(self.cluster_diameter))\n",
        "        #print(\"# sum_dict:\")\n",
        "        #print(self.sum_dict)\n",
        "        #print(\"# prd_dict:\")\n",
        "        #print(self.prd_dict)\n",
        "        #print(\"# corr_dict:\")\n",
        "        #print(self.corr_dict)\n",
        "        #print(\"# rnomc_dict:\")\n",
        "        #print(self.rnomc_dict)\n",
        "        #print(\"# hoeffding_bound = {}\".format(self.hoeffding_bound))\n",
        "\n",
        "\n",
        "class Cluster:\n",
        "\n",
        "    #def __init__(self, confidence_level = 0.9, n_min = 5, tau = 0.01):\n",
        "    def __init__(self, confidence_level = 0.9, n_min = 5, tau = 0.1):\n",
        "        self.active_cluster = True\n",
        "        self.statistics = None\n",
        "        self.confidence_level = confidence_level\n",
        "        self.n_min = n_min\n",
        "        self.tau = tau\n",
        "\n",
        "\n",
        "    def set_cluster_timeseries(self,list_ts):\n",
        "        #self.list_of_timeseries = OrderedDict(sorted(list_ts.items(), key=lambda t: t[0]))\n",
        "        self.list_of_timeseries = list_ts\n",
        "        \n",
        "        cluster_size = len(self.list_of_timeseries)\n",
        "        self.statistics = Statistics(cluster_size)\n",
        "        self.update_statistics(init=True) ### does not generate ts samples only calculate matrices\n",
        "\n",
        "\n",
        "    def get_cluster_timeseries(self):\n",
        "      return self.list_of_timeseries\n",
        "\n",
        "\n",
        "    def list_timeseries_names(self):\n",
        "      return list(self.list_of_timeseries.keys())\n",
        "      \n",
        "    def list_timeseries_values(self):\n",
        "      return list(self.list_of_timeseries.values())\n",
        "\n",
        "\n",
        "    def calcula_sum_dict(self,init=False):\n",
        "\n",
        "        for k in self.statistics.sum_dict:\n",
        "            self.statistics.sum_dict[k] = self.statistics.sum_dict[k] \\\n",
        "                + list(self.list_of_timeseries.values())[k].current_value\n",
        "\n",
        "        return self.statistics.sum_dict\n",
        "\n",
        "\n",
        "    def calcula_prod_dict(self,init=False):\n",
        "\n",
        "        for k in self.statistics.prd_dict:\n",
        "            self.statistics.prd_dict[k] = self.statistics.prd_dict[k] \\\n",
        "                + ( list(self.list_of_timeseries.values())[k[0]].current_value \\\n",
        "                * list(self.list_of_timeseries.values())[k[1]].current_value )\n",
        "\n",
        "        return self.statistics.prd_dict\n",
        "\n",
        "    def calcula_corr_dict(self,init=False):\n",
        "\n",
        "        for k in self.statistics.corr_dict:\n",
        "            i = k[0]\n",
        "            j = k[1]\n",
        "            p = self.statistics.prd_dict[(i,j)]\n",
        "            a = self.statistics.sum_dict[i]\n",
        "            a2 = self.statistics.prd_dict[(i,i)]\n",
        "            b = self.statistics.sum_dict[j]\n",
        "            b2 = self.statistics.prd_dict[(j,j)]\n",
        "            n = self.statistics.n_of_instances\n",
        "            \n",
        "            a = a.mean()\n",
        "            a2 = a2.mean()\n",
        "            b = b.mean()\n",
        "            b2 = b2.mean()\n",
        "            \n",
        "            term_p = p - ((a*b)/n)\n",
        "            \n",
        "            term_a = sqrt(a2 - ((a*a)/n))\n",
        "            term_b = sqrt(b2 - ((b*b)/n))\n",
        "\n",
        "            self.statistics.corr_dict[(i,j)] = term_p/(term_a*term_b)\n",
        "\n",
        "        return self.statistics.corr_dict\n",
        "\n",
        "\n",
        "    def calcula_rnomc_dict(self,init=False):\n",
        "      max_rnomc = None\n",
        "      for k in self.statistics.rnomc_dict:\n",
        "        \n",
        "        c = 1 - self.statistics.corr_dict[k]\n",
        "        \n",
        "        self.statistics.rnomc_dict[k] = sqrt(c.mean())# / 2 )\n",
        "        \n",
        "        if max_rnomc is None or self.statistics.rnomc_dict[k] > max_rnomc:\n",
        "          max_rnomc = self.statistics.rnomc_dict[k]\n",
        "\n",
        "      self.cluster_diameter = max_rnomc\n",
        "      \n",
        "      return self.statistics.rnomc_dict\n",
        "\n",
        "    def calcula_distances_coefficients(self):\n",
        "      rnorm_copy = self.statistics.rnomc_dict.copy()\n",
        "      \n",
        "      #print(rnorm_copy)\n",
        "      if (rnorm_copy == {}):\n",
        "        rnorm_copy = {'': 0}\n",
        "      \n",
        "      self.statistics.dist_dict_coef['avg'] = sum(rnorm_copy.values()) / len(rnorm_copy)\n",
        "\n",
        "      d0_pair = min(rnorm_copy, key=rnorm_copy.get)\n",
        "      d0 =  rnorm_copy[min(rnorm_copy, key=rnorm_copy.get)]\n",
        "\n",
        "      self.statistics.dist_dict_coef['d0_val'] = d0\n",
        "      self.statistics.dist_dict_coef['d0_pair'] = d0_pair\n",
        "\n",
        "      d1_pair = max(rnorm_copy, key=rnorm_copy.get)\n",
        "      d1 =  rnorm_copy[max(rnorm_copy, key=rnorm_copy.get)]\n",
        "\n",
        "      self.statistics.dist_dict_coef['d1_val'] = d1\n",
        "      self.statistics.dist_dict_coef['d1_pair'] = d1_pair\n",
        "\n",
        "      rnorm_copy.pop(d1_pair, None)\n",
        "\n",
        "      if (rnorm_copy):\n",
        "          ## in case rnorm dict has only one or 2 elements. in that case\n",
        "          ## we are calculating this but we are not allowed to split.\n",
        "          ## TODO >>> we can also check the rnorm lenght in the begining.\n",
        "\n",
        "          d2_pair =  max(rnorm_copy, key=rnorm_copy.get)\n",
        "          d2 =   rnorm_copy[max(rnorm_copy, key=rnorm_copy.get)]\n",
        "          self.statistics.dist_dict_coef['d2_val'] = d2\n",
        "          self.statistics.dist_dict_coef['d2_pair'] = d2_pair\n",
        "          self.statistics.dist_dict_coef['delta'] = d1-d2\n",
        "      else:\n",
        "          self.statistics.dist_dict_coef['d2_val'] = None\n",
        "          self.statistics.dist_dict_coef['d2_pair'] = None\n",
        "          self.statistics.dist_dict_coef['delta'] = None\n",
        "\n",
        "\n",
        "    def calcula_hoeffding_bound(self,init=False):\n",
        "\n",
        "        r_sqrd = 1  # because the data is normalized\n",
        "        self.statistics.hoeffding_bound = sqrt(r_sqrd * log(1/self.confidence_level) \\\n",
        "            / (2 * self.statistics.n_of_instances))\n",
        "\n",
        "        return self.statistics.hoeffding_bound\n",
        "\n",
        "\n",
        "    def update_statistics(self , init=False):\n",
        "\n",
        "        if init == False:\n",
        "            self.get_new_timeseries_values()\n",
        "\n",
        "        #print(\"### new observation:\", end='')\n",
        "        #for ts in list(self.list_of_timeseries.values()):\n",
        "        #    print(\", {}\".format(ts.current_value), end='')\n",
        "\n",
        "        self.statistics.n_of_instances += 1\n",
        "\n",
        "        ### calculate Matrice\n",
        "        self.calcula_sum_dict()\n",
        "\n",
        "        ### calculate prod matrix\n",
        "        self.calcula_prod_dict()\n",
        "\n",
        "        if self.statistics.n_of_instances >= self.n_min:\n",
        "\n",
        "            ### calculate Matrice coor\n",
        "            self.calcula_corr_dict()\n",
        "\n",
        "            ### calculate Matrice dif\n",
        "            self.calcula_rnomc_dict()\n",
        "\n",
        "        # hoeffding bound as epsilon proposed in 3.4.1\n",
        "        self.calcula_hoeffding_bound()\n",
        "\n",
        "        # distance parameters needed for checks in 3.4.1 and 3.4.3 of the paper\n",
        "        self.calcula_distances_coefficients()\n",
        "\n",
        "\n",
        "        #self.statistics.print()\n",
        "        #print(\"\")\n",
        "\n",
        "\n",
        "    def get_new_timeseries_values(self):\n",
        "        for ts in self.list_of_timeseries.values():\n",
        "            ts.next_val()\n",
        "\n",
        "\n",
        "    def get_smaller_distance_with_pivot(self, pivot_1, pivot_2, current):\n",
        "        \"\"\" Look for the distance\n",
        "        in rnomc_dict given 2 pivots and an index\n",
        "        \"\"\"\n",
        "\n",
        "        ## Following the rule that the dict key present a tuple(x,y) where x < y\n",
        "        ## here the method max and min are used to find the correct poition\n",
        "        #print(\"{}\\n{} {} (min={} max={})\".format(len(self.statistics.rnomc_dict), pivot_1, current, min(pivot_1,current), max(pivot_1,current)))\n",
        "        dist_1 = self.statistics.rnomc_dict[(min(pivot_1,current),max(pivot_1,current))]\n",
        "        #print(\"{}\\n{} {} (min={} max={})\".format(len(self.statistics.rnomc_dict),pivot_2, current, min(pivot_2,current), max(pivot_2,current)))\n",
        "        dist_2 = self.statistics.rnomc_dict[(min(pivot_2,current),max(pivot_2,current))]\n",
        "\n",
        "        return 2 if dist_1 >= dist_2 else 1\n",
        "\n",
        "\n",
        "    def split_this_cluster(self, pivot_1, pivot_2):\n",
        "\n",
        "        pivot_1_list = {}\n",
        "\n",
        "        temp_1 = list(self.list_of_timeseries.items())[pivot_1]\n",
        "        pivot_1_list[temp_1[0]] = temp_1[1]\n",
        "\n",
        "        pivot_2_list = {}\n",
        "\n",
        "        temp_2 = list(self.list_of_timeseries.items())[pivot_2]\n",
        "        pivot_2_list[temp_2[0]] = temp_2[1]\n",
        "\n",
        "        for i in range(len(self.list_of_timeseries.values())):\n",
        "            if (i != pivot_1) & (i != pivot_2):\n",
        "                cluster = self.get_smaller_distance_with_pivot(pivot_1, pivot_2, i)\n",
        "                if cluster == 1:\n",
        "                  print(self.list_of_timeseries.items())\n",
        "                  temp_1 = list(self.list_of_timeseries.items())[i]\n",
        "                  print(temp_1)\n",
        "                  pivot_1_list[temp_1[0]] = temp_1[1]\n",
        "                else: #2\n",
        "                  print(self.list_of_timeseries.items())\n",
        "\n",
        "                  temp_2 = list(self.list_of_timeseries.items())[i]\n",
        "                  print(temp_2)\n",
        "                  pivot_2_list[temp_2[0]] = temp_2[1]\n",
        "\n",
        "        ### After creating 2 lists based on 2 pivots, its time\n",
        "        ### to creates the new nodes and set self as the parent node\n",
        "\n",
        "        ## this is just to create the node name.\n",
        "        ## this is irrelevant...\n",
        "        if(self.name == 'root_node'):\n",
        "            new_name = \"1\"\n",
        "        else:\n",
        "            new_name = str(int(self.name[8:]) + 1)\n",
        "            print(new_name)\n",
        "\n",
        "        cluster_child_1 = Node_of_tree('CH1_LVL_'+new_name, parent=self)\n",
        "        cluster_child_1.set_cluster_timeseries(pivot_1_list)\n",
        "\n",
        "        cluster_child_2 = Node_of_tree('CH2_LVL_'+new_name, parent=self)\n",
        "        cluster_child_2.set_cluster_timeseries(pivot_2_list)\n",
        "\n",
        "        ### Finally, the current cluster is deactivated.\n",
        "\n",
        "        self.active_cluster = False\n",
        "\n",
        "\n",
        "    def aggregate_this_cluster(self):\n",
        "        self.parent.statistics.reset_sufficient_statistics(len(self.list_of_timeseries))\n",
        "        self.children = []\n",
        "        self.active_cluster = True\n",
        "\n",
        "\n",
        "    def test_split(self):\n",
        "        #print('test split: {} >= {} and {} not None'.format(self.statistics.n_of_instances, self.n_min, self.statistics.dist_dict_coef['d2_val']))\n",
        "        if self.statistics.n_of_instances >= self.n_min and self.statistics.dist_dict_coef['d2_val'] is not None:\n",
        "            d0 = float(self.statistics.dist_dict_coef['d0_val'])\n",
        "            d1 = float(self.statistics.dist_dict_coef['d1_val'])\n",
        "            d2 = float(self.statistics.dist_dict_coef['d2_val'])\n",
        "            avg = float(self.statistics.dist_dict_coef['avg'])\n",
        "            t = float(self.tau)\n",
        "            e =  float(self.statistics.hoeffding_bound)\n",
        "\n",
        "            \n",
        "            \n",
        "            ### following 3.4.4 Split Algorithm\n",
        "            if ( (d1 - d2) > e ) | ( t > e ):\n",
        "                #print(\"### FIRST CONDITION MET\")\n",
        "                \n",
        "                if ( (d1 - d0) * abs((d1 - avg) - (avg - d0)) ) > e:\n",
        "\n",
        "                    x1 = self.statistics.dist_dict_coef['d1_pair'][0]\n",
        "                    y1 = self.statistics.dist_dict_coef['d1_pair'][1]\n",
        "\n",
        "                    print(\"#################\")\n",
        "                    print(\"##### SPLIT #####\")\n",
        "                    print(\"#################\")\n",
        "                    print(\" >>> PIVOT: \", x1, y1)\n",
        "                    print(\"#################\")\n",
        "\n",
        "                    self.split_this_cluster(pivot_1 = x1, pivot_2 = y1)\n",
        "\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    def test_aggregate(self):\n",
        "        if self.parent is not None:\n",
        "\n",
        "            if self.statistics.dist_dict_coef['d1_val'] - self.parent.statistics.dist_dict_coef['d1_val'] \\\n",
        "                > max(self.statistics.hoeffding_bound, self.parent.statistics.hoeffding_bound):\n",
        "\n",
        "                print(\"#################\")\n",
        "                print(\"##### AGGR. #####\")\n",
        "                print(\"#################\")\n",
        "\n",
        "                #print(\"checking: c_k={} c_j={} e_k={} e_j={}\".format( \\\n",
        "                #    self.statistics.dist_dict_coef['d1_val'], \\\n",
        "                #    self.parent.statistics.dist_dict_coef['d1_val'], \\\n",
        "                #    self.statistics.hoeffding_bound, \\\n",
        "                #    self.parent.statistics.hoeffding_bound))\n",
        "\n",
        "                self.parent.aggregate_this_cluster()\n",
        "\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Node_of_tree(Cluster, NodeMixin):  # Extension to class Cluster to use tree structure\n",
        "    def __init__(self, name, parent=None, children=None):\n",
        "        super(Node_of_tree, self).__init__()\n",
        "        self.name = name\n",
        "        self.parent = parent\n",
        "        if children:\n",
        "             self.children = children"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg_D0EgK6UUb",
        "colab_type": "text"
      },
      "source": [
        "#main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u36a_Pja-i64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from math import ceil\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def save_plt(sl, figname):\n",
        "  \n",
        "  \n",
        "  for l in sl:\n",
        "    colors = cm.rainbow(np.linspace(0, 1, len(l)))\n",
        "    #xs = [df.x.values[:n], df.x.values[n:2*n], df.x.values[2*n:]]\n",
        "    #ys = [df.y.values[:n], df.y.values[n:2*n], df.y.values[2*n:]]\n",
        "    xs, ys = zip(*l)\n",
        "\n",
        "    for x, y, color in zip(xs, ys, colors):\n",
        "      plt.scatter(x, y, c=[color.tolist()])\n",
        "\n",
        "  plt.title('ODAC')\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.savefig(figname + '.png')\n",
        "  plt.clf()\n",
        "  plt.cla()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb3AzjNIXNoS",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "from anytree.search import findall\n",
        "from anytree import RenderTree\n",
        "import sys\n",
        "\n",
        "series = generate_multi_timeseries(n=100)\n",
        "\n",
        "### Initialize root node of the tree ( the initial cluster )\n",
        "### and tet timeseries to initial cluster\n",
        "root_node = Node_of_tree('root_node')\n",
        "root_node.set_cluster_timeseries(series)\n",
        "\n",
        "### example run 10 times:\n",
        "### for each active cluster - get next value for each series inside the cluster and\n",
        "### calculate and update statistics\n",
        "\n",
        "sl = []\n",
        "#fig_idx = 0\n",
        "\n",
        "for i in range(10):\n",
        "  for active_cluster in findall(root_node,filter_=lambda node: node.active_cluster is True):\n",
        "    active_cluster.update_statistics()\n",
        "    if active_cluster.test_split() or active_cluster.test_aggregate():\n",
        "      print(\"tree at observation #{}\".format(i))\n",
        "      for pre, fill, node in RenderTree(root_node):\n",
        "        print(\"%s%s %f %s\" % (pre, node.name, node.statistics.dist_dict_coef['d1_val'], node.list_timeseries_names() if node.active_cluster else \" [NOT ACTIVE]\" ))\n",
        "\n",
        "        if node.active_cluster:\n",
        "          for v in node.list_timeseries_values():\n",
        "            sl.append(v)\n",
        "          #save_plt(sl, str(fig_idx))\n",
        "          #fig_idx += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjbqdyDndIdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for l in sl:\n",
        "  print(len(l))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebfwmr4nAi3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar cvf images.tar \n",
        "\n",
        "from google.colab import files\n",
        "files.download('images.tar') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CWoGk4iAnIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from math import ceil\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "\n",
        "# Create data\n",
        "N = df.x.size\n",
        "n = int(ceil(N/3))\n",
        "\n",
        "colors = cm.rainbow(np.linspace(0, 1, n))\n",
        "\n",
        "#print([x[:3] for x in colors.tolist()])\n",
        "xs, ys = df.x.values, df.y.values\n",
        "#print(xs.values)\n",
        "#for l in range(n):\n",
        "#  xs = [df.x.values[:n], df.x.values[n:2*n], df.x.values[2*n:]]\n",
        "#  ys = [df.y.values[:n], df.y.values[n:2*n], df.y.values[2*n:]]\n",
        "#  xs, ys = zip(*l)\n",
        "\n",
        "# Plot\n",
        "for x, y, color in zip(xs, ys, colors):\n",
        "  plt.scatter(x, y, c=[color.tolist()])\n",
        "\n",
        "plt.title('ODAC')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.savefig('teste.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE9JjgPKdE3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('teste.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwuVIFZyaxZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}